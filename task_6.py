# -*- coding: utf-8 -*-
"""Task_6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DV0Uo436oQnjUtGRbiTMuRvR_F-DJzdS

# Task 6 -Building a Convolutional neural network

Azucena Ascencio-Cabral

Features:
- Batch Normalisation
- Dropout
- Adam and SGD otimizers
- Learning rate scheduler
"""

from google.colab import drive
drive.mount('/content/drive')

"""## Importing libraries"""

import os,sys
import numpy as np
import torch
import torchvision
from torchvision import transforms as T
import torch.nn as nn
import torch.nn.functional as F
from torch import nn
from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau
import torch.optim as optim 
import pickle
import time
import copy
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from torch.utils.data import Dataset, DataLoader, Subset
from PIL import Image

"""## Training on CUDA or CPU"""

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

"""### Opening the file"""

with open(("/content/drive/My Drive/ck_final.pickle"), "rb") as fh:
    data = pickle.load(fh)
print(data.keys())

print(data['img_dim'])

"""## Data Visualisation"""

plt.imshow(data['training_data'][0][200].reshape((100, 100)), cmap="gray")
print(data['training_data'][0].shape)

"""### Review of the dataset and concatenation
The daset was obtained  from a picklefile. Therefore I will concatenate the splits and proceed with random permutatation to ensure the splits have a even distribution of difficult and easy emotions.
"""

X_train = data['training_data'][0]
y_train =data['training_data'][1]
X_val = data['validation_data'][0]
y_val =data['validation_data'][1]
X_test= data['test_data'][0]
y_test = data['test_data'][1]

X= np.append(X_train, X_val, axis = 0)
X= np.append(X, X_test, axis = 0)
y = np.append(y_train, y_val, axis = 0)
y= np.append(y, y_test, axis = 0)

# Verification of the number of images and labels

print("Labels",len(y))
print("Images",len(X))

# Verifiction of the number of emotions
np.unique(y)

# Creating dictionary for the emotions tracking
emotions = ["neutral", "anger", "contempt", "disgust", "fear", "happy", "sadness", "surprise"]

label ={}
for n, emotion in enumerate (emotions):
    label[n] = emotion
print(label)

"""### Customised dataset class

Here a customised dataset class for thr Cohn-Kanade dataset will be created.
"""

class CKDataset(Dataset):

    def __init__(self, images, labels, transforms=None):
        self.images = X
        self.labels = y
        self.transforms = transforms

        assert len(images) == len(labels), 'Length of data and label should be same'
        self.length = len(images)

    def __len__(self):
        return self.length

    def __getitem__(self, idx):
        img = self.images[idx]
        target = int(self.labels[idx])
        img = np.reshape(img, [100, 100,1]) # Reshape for convolutional

        target = int(self.labels[idx])
        if self.transforms is not None:
            img = self.transforms(img)

        return img, target

"""###  Build a neural network class
A customised  a convolutional neural network class with droput and batch normalisation options will be built. Dilation and padding will be zero.





#### Output Formula for Convolution
- $ Wout = \frac {Win + 2P[1] - D[1] - K[1]}{S[1]} + 1$
    - $Wout$: output heigh/length
    - $Win$: input height/length
    - $D$: input height/length
    - $K$: filter size (kernel size) = 5
    - $P$: zero padding
     - $ P = \frac{K - 1}{2} $ if padding non-zero
    - $S$: stride
    - $S$: **stride = 1**

    
#### Output Formula for Pooling
- $ O = \frac {W}{K}$
    - W: input height/width
    - K: **filter size = 2**

Here the input width and heigth are (100,100) pixels



-  conv1 = (100-5+ 2(0))/1-1 = 96 for the first Conv1 
- maxpool1 =  96/2 =  48
- conv2 = (48 -5+ 2(0))/1-1 = 44
- maxpoo12 =44/2 = 22
- fc1 in = 22X22X50 = 24200  # 50 is an arbitary output chanels from conv2 -input for fc1
- fc2 in = 200 # arbitraty number of out channels from fc1 (input for fc2)

For the putrpose of this work batch normalization and droput can be undertaken using the same architectute. Probabilities need to be zeroed when batch normalization is "True". A system exit will be triggered and a message displayed to prompt either bacth normalization or droput. If dropout or batch normalisation are not undertaken, regularization is still possible by using L2 regularization (weight decay) with the optimizer.

**Note:** Comment the system exit if running dropout with batch normalisation
"""

class ANN(nn.Module):
    def __init__(self, batch_norm = False, is_training = True): #p1,p2,
        super(ANN, self).__init__()
      
        self.training = is_training
        self.batch_norm = batch_norm
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=20, 
                               kernel_size=5, stride=1)
        self.conv2 = nn.Conv2d(in_channels=20, out_channels=50, 
                               kernel_size=5, stride=1)
        if batch_norm:
            self.conv2_bn = nn.BatchNorm2d(50)
            self.fc1_bn = nn.BatchNorm1d(200) # out features
        self.conv2_drop = nn.Dropout2d(p=p1) #p1=0.4
        self.fc1 = nn.Linear(in_features=24200, out_features= 200)  #22*22*50
        self.fc1_drop = nn.Dropout2d(p=p2) #p2=05
        self.fc2 = nn.Linear(in_features=200, out_features= 8) #8 number of classes


        # if self.batch_norm == True and p1 > 0.0 and p2 > 0.0:
        #   sys.exit("If batch is true zero droput probabilities p1 and p2")
        # if self.batch_norm == True and p1 == 0 and p2 != 0.0:
        #   sys.exit("If batch  is true zero droput probabilities p1 and p2")
        # if self.batch_norm == True and p1 != 0 and p2 == 0 :
        #   sys.exit("If batch is true zero droput probabilities p1 and p2")

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.max_pool2d(x, 2, 2)
        # x= self.conv2(x)

        if batch_norm:
            x = self.conv2_bn(self.conv2(x))
            x = F.relu(x)
            x = F.max_pool2d(x, 2, 2)
            x = self.conv2_drop(x)
            x = x.view(-1, 24200)
            x= self.fc1(x)
            x= self.fc1_bn(x)
            x= F.relu(x)
            x = self.fc1_drop(x)

        else:
            x = F.relu(self.conv2(x))
            x = F.max_pool2d(x, 2, 2)
            x = self.conv2_drop(x)
            x = x.view(-1, 24200)
            x = F.relu(self.fc1(x))

            x = self.fc1_drop(x)         
        x = self.fc2(x)
      
        return x

"""### Creating the dataset"""

# Function to prevent warning caused by PILImage- no. writabletensor. - From pytorch forums

class ToNumpy(object):
    def __call__(self, img):
        return np.array(img)

# Data augmentation to the training data will be undertaken - T.RandomHorizontalFlip

transform_train = T.Compose([T.ToPILImage(), T.RandomHorizontalFlip(0.5), ToNumpy(), T.ToTensor()])
# Only tensor transformations will be done to the test and validation datasets
transform = T.Compose([T.ToTensor()])

# # dataset and defined transformations
train_dataset = CKDataset(X, y, transforms = transform_train)
test_dataset = CKDataset(X, y, transforms= transform)

# split the dataset in train, validation and test datasets using torch random permutations
indices = torch.randperm(len(train_dataset)).tolist()
idx= round(len(indices)*0.70)
train_idx = round(len(indices)*0.85)
train_dataset = torch.utils.data.Subset(train_dataset, indices[:idx])
val_dataset =   torch.utils.data.Subset(test_dataset, indices[idx:train_idx])
test_dataset = torch.utils.data.Subset(test_dataset, indices[train_idx:])

"""### Loss function and optimizer"""

# loss funcion- cross entropy-softmax
criterion = nn.CrossEntropyLoss()

# Defining SGD and Adam optimizers

def optim (model, opt, lr, momentum, w_decay):
    if opt == "Adam":
        opts= torch.optim.Adam(model.parameters(), lr= learning_rate, weight_decay= w_decay)
    if opt =="SGD":
        opts = torch.optim.SGD(model.parameters(), lr= learning_rate, momentum=mom,
                      weight_decay= w_decay)
    return opts

"""### Building the training and evaluation functions"""

def train(model, loader, criterion, optimizer ):
    model.train()
    loss_sum, correct_sum  = 0, 0
    for idx, (inputs, labels) in enumerate(loader):
        # data pixels and labels to available device
        inputs, labels = inputs.to(device), labels.to(device)
        # set the parameter gradients to zero
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels.long())
        loss_sum += loss.item()*inputs.size(0)
        # propagate the loss backward
        loss.backward()
        # update the gradients
        optimizer.step()
        _, preds = torch.max(outputs, 1)
        correct_sum += torch.sum(preds == labels) 
    #Statistics 
    train_loss = loss_sum/len(loader.dataset)
    train_acc = correct_sum.float()/len(loader.dataset)

    return train_loss, train_acc


def test_val (model, loader, criterion):
    model.eval()
    test_loss_sum, correct_sum  = 0, 0
    with torch.no_grad():
        loss_sum, correct_sum , total = 0, 0, 0
        for idx, (inputs, labels) in enumerate(loader):
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels.long())
            loss_sum +=  loss.item()*inputs.size(0)
            _, preds = torch.max(outputs, 1) 
            correct_sum += torch.sum(preds == labels)   
        #Statistics 
        loss = loss_sum/len(loader.dataset)
        acc = correct_sum.float()/len(loader.dataset)
        
    return loss, acc

"""## Building the main - running fucntion"""

# Commented out IPython magic to ensure Python compatibility.
def main(model, criterion, lr_scheduler, num_epochs):
    since = time.time()

    best_acc = 0.0

    train_loss_log= []
    train_corrects_log = []
    val_loss_log= []
    val_corrects_log = []
    test_loss_log= []
    test_corrects_log = []

    for epoch in range(num_epochs):
        epoch_strlen = len(str(num_epochs)) 
        #getting the loss and accurracy from the training, validation and test datasets
        train_loss, train_acc = train(model, train_loader, criterion, optimizer)
        # lr_scheduler.step()
        val_loss, val_acc = test_val(model, val_loader, criterion)
        test_loss, test_acc = test_val(model, test_loader, criterion)
        if test_acc > best_acc:
            best_acc = test_acc
            best_epoch= int(epoch + 1)
        #     best_model = copy.deepcopy(model.state_dict())

        if lr_scheduler== lr_schedulerpl:
            lr_scheduler.step(test_acc)
        else:
          lr_scheduler.step()
  
        sys.stderr.write('\r%0*d/%d | Train / Val/ Test loss.: %.3f / %.3f / %.3f '' | Train/Val/Test Acc.: %.3f%%/ %.3f%%/ %.3f%% ' 
#                     % (epoch_strlen, epoch+1,num_epochs, train_loss, val_loss, test_loss, 
                      train_acc*100, val_acc*100, test_acc*100))
        print('| lr.:{:5f}'.format((optimizer.param_groups[0]['lr'])))
        
    
        train_loss_log.append(train_loss)
        train_corrects_log.append(train_acc)
        val_loss_log.append(val_loss)
        val_corrects_log.append(val_acc)
        test_loss_log.append(test_loss)
        test_corrects_log.append(test_acc)

    time_elapsed = time.time() - since
    print('Training complete in {:.0f}m {:.0f}s'.format(
        time_elapsed // 60, time_elapsed % 60))
    print('Best test Acc: {:3f}'.format(best_acc*100))
    print('Best test epoch: {:2d}'.format(best_epoch))


    return train_loss_log, train_corrects_log, val_loss_log, val_corrects_log, test_loss_log, test_corrects_log

"""## Preparing the model for training and evaluation

### Hyper parameters

Set gamma to 1 if not learning decay applied
Set p1 and p0 to run without droput
set batch_norm to "False" or "True" if using or not bach normlisation
"""

num_epochs = 60
batch_size = 10 # 110
batch_norm = True
learning_rate = 0.005

mom = 0 # No neededed for Adam optimizer
w_decay =0.001
p1, p2 = 0.5, 0.5
opt = "Adam"  # Select "Adam" or "SGD"

############
#Learning params 
###############
# step learning rate adjustment
step_size = 20
gamma = 0.5 
# plateu learning rate adjustment
factor = 0.2 
patience = 2

"""### Printing the model"""

# Printing the neural network model - move the model to the available device
ann_model = ANN(batch_norm = batch_norm, is_training= True ).to(device)
# Move the model to the available device
ann_model.to(device)

print(ann_model)

"""###Â Build the data loaders and optimizer"""

# Data Loader
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size= batch_size, shuffle=True)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size= batch_size, shuffle=False)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size= batch_size, shuffle=False)

optimizer = optim(ann_model, opt= opt, lr=learning_rate, momentum=mom, w_decay=w_decay)

print(optimizer)

"""### Learning scheduler"""

#Defining learning scheduler -step

lr_schedulerst = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size= step_size, 
                                               gamma=gamma)

#Learning scheduler plateu
lr_schedulerpl = ReduceLROnPlateau(optimizer=optimizer, mode='max',factor= factor, 
                                   patience= patience, verbose= True)

"""## Runing the model"""

train_loss_log, train_corrects_log, val_loss_log, val_corrects_log,test_loss_log, test_corrects_log = main(ann_model, criterion, lr_scheduler= lr_schedulerpl, num_epochs=num_epochs)

"""## Statistics visualisation

#### Cost
"""

#Visualising the cost vs epoc

plt.plot(train_loss_log, label='training loss')
plt.plot(val_loss_log, label='validation loss')
plt.plot(test_loss_log, label='test loss')
plt.legend()

"""#### Accuracy"""

#Visualising the accuracy vs epoch

plt.plot(train_corrects_log, label='training accuracy')
plt.plot(val_corrects_log, label='val accuracy')
plt.plot(test_corrects_log, label='test accuracy')

"""### Saving the model

Alternatively the model can be saved. Change the path to your personal path and uncomment the cell.
"""

path = '/content/drive/My Drive/ck.pth'
torch.save(ann_model.state_dict(), path)

"""### Visualising the test data"""

def imshow(img):
    img = img / 2 + 0.5     # unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()

dataiter = iter(test_loader)
images, labels = dataiter.next()

# print images
fig = plt.figure(figsize=(13, 7))
imshow(torchvision.utils.make_grid(images))
print('GroundTruth: ', ' '.join('%5s' % emotions[labels[j]] for j in range(batch_size)))

"""### Accessing the saved model -optional

Uncomment this cell if accessing the saved model
"""

ann_model.state_dict(torch.load(path)) # Change the path to your directory

outputs =ann_model(images.to(device))

_, predicted = torch.max(outputs, 1)

print('Predicted: ', ' '.join('%5s' % emotions[predicted[j]]
                              for j in range(batch_size)))

# correct = 0
# total = 0
# with torch.no_grad():
#     for data in test_loader:
#         images, labels = data
#         images, labels = images.to(device), labels.to(device)
#         outputs = ann_model(images)
#         _, predicted = torch.max(outputs.data, 1)
#         total += labels.size(0)
#         correct += (predicted == labels).sum().item()

# print('Accuracy of the network on the test images: %.2f %%' % (
#     100 * correct / total))

# # Move the model to the available device
# ann_model.to(device)

"""### Statistics per label - test dataset"""

class_correct = list(0. for i in range(len(emotions)))
class_total = list(0. for i in range(len(emotions)))
with torch.no_grad():
    for data in test_loader:
        inputs, labels = data
        inputs = inputs.to(device)
        labels = labels.to(device)

        outputs = ann_model(inputs)
        _, predicted = torch.max(outputs, 1)
        correct = (predicted == labels).squeeze()
        if correct.ndim == 0:
            continue
        for i in range(len(test_loader.dataset)):
            if i < len(labels):
                label = labels[i].item()
                class_correct[label] += correct[i].item()
                class_total[label] += 1
for i in range(len(emotions)):
    print('Accuracy of %5s : %.2f %%' % (
        emotions[i], 100 * class_correct[i] / class_total[i]))